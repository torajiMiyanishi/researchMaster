{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "generateGtfsFile.ipynb\n",
    "preparation.json\n",
    "preparation.pickle\n",
    "をもとに.gtfsデータを構築する\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "\"\"\"\n",
    "FUNCTION\n",
    "\"\"\"\n",
    "def json_to_dict(dir, encoding_=\"utf-8\"):\n",
    "    \"\"\"\n",
    "    @dir        : 読み込み先ファイルパス\n",
    "    @encoding_  : 読み込むファイルのエンコードによって適宜変える。\n",
    "    \"\"\"\n",
    "    with open(dir, mode=\"rt\", encoding=encoding_) as f:\n",
    "        data = json.load(f)\t\t# JSONのファイル内容をdictに変換する。\n",
    "        return data\n",
    "      \n",
    "def add_set(added_element,set_:set):\n",
    "    \"\"\"\n",
    "    @added_element:追加される要素\n",
    "    @added_set:追加されるセット\n",
    "    \"\"\"\n",
    "    before_size = len(set_)\n",
    "    set_.add(added_element)\n",
    "    after_size = len(set_)\n",
    "    return before_size != after_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INPUT\n",
    "\"\"\"\n",
    "# 作業ディレクトリの定義\n",
    "DIR_WRITING = \"C:/lab/gtfses/Ishigaki_pj\"\n",
    "# 基本情報のjsonを取得\n",
    "BASIC_INFO = json_to_dict(DIR_WRITING + \"/material.json\")\n",
    "BASIC_INFO\n",
    "# バス会社のagency idの取得\n",
    "agencyId = str(BASIC_INFO[\"URL_BUS_COMPANY_PAGE\"].split(\"/\")[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gtfsフォルダの作成\n",
    "\"\"\"\n",
    "PathToGdfs = DIR_WRITING+\"/\" + agencyId +\".gtfs\"\n",
    "Path(PathToGdfs).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "agency.txtの作成\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"agency_id\"\"\"\n",
    "agency_id = agencyId\n",
    "\n",
    "\"\"\"agency_name\"\"\"\n",
    "response = requests.get(BASIC_INFO[\"URL_BUS_COMPANY_PAGE\"])\n",
    "agency_name = BeautifulSoup(response.text,\"html.parser\").find(\"h1\",class_=\"main-title\").text.split(u\"\\xa0\")[0]\n",
    "\n",
    "\"\"\"agency_url\"\"\"\n",
    "agency_url = BASIC_INFO[\"URL_BUS_COMPANY_PAGE\"]\n",
    "\n",
    "\"\"\"agency_timezone\"\"\"\n",
    "agency_timezone = \"Asia/Tokyo\"\n",
    "\n",
    "\"\"\"agency_lang\"\"\"\n",
    "agency_lang = \"ja\"\n",
    "\n",
    "\"\"\".txtを作成\"\"\"\n",
    "pd.DataFrame({\"agency_id\":[agency_id],\"agency_name\":[agency_name],\"agency_url\":[agency_url],\"agency_timezone\":[agency_timezone],\"agency_lang\":[agency_lang]}).to_csv(PathToGdfs+\"/agency.txt\",index=False,sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "stops.txtの作成\n",
    "\"\"\"\n",
    "column_dict = {\n",
    "    \"nameBS_navitime\": \"stop_name\",\n",
    "    \"id\": \"stop_id\",\n",
    "    \"lon\": \"stop_lon\",\n",
    "    \"lat\": \"stop_lat\"\n",
    "}\n",
    "tableBS = pd.read_pickle(DIR_WRITING+\"/material.pickle\").rename(column_dict,axis=1).filter(items=[\"stop_id\",\"stop_name\",\"stop_lat\",\"stop_lon\"])\n",
    "tableBS.to_csv(DIR_WRITING+\"/\"+ agencyId +\".gtfs/stops.txt\",index=False,sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "infoBS = pd.read_pickle(DIR_WRITING+\"/material.pickle\")\n",
    "url_ = \"https:\"+list(infoBS.at[7,\"url\"])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "routeNameSets = ()\n",
    "html_text = requests.get(url_).text\n",
    "soup = BeautifulSoup(html_text,\"html.parser\")\n",
    "lineLists = soup.find_all(\"dl\",class_=\"line-list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[9]川平リゾート線[東運輸]': [{'クラブメッド方面': '//www.navitime.co.jp/diagram/bus/00428185/00070166/1/'},\n",
       "  {'バスターミナル（石垣市）方面': '//www.navitime.co.jp/diagram/bus/00428185/00070166/0/'}],\n",
       " '[10]ANAコンチネンタル経由空港線[東運輸]': [{'ＡＮＡインターコンチネンタル（石垣市）/石垣空港（バス）方面': '//www.navitime.co.jp/diagram/bus/00428185/00070167/1/'},\n",
       "  {'バスターミナル（石垣市）方面': '//www.navitime.co.jp/diagram/bus/00428185/00070167/0/'}],\n",
       " '[11]米原キャンプ場線[東運輸]': [{'クラブメッド/石垣空港（バス）方面': '//www.navitime.co.jp/diagram/bus/00428185/00070170/1/'},\n",
       "  {'バスターミナル（石垣市）方面': '//www.navitime.co.jp/diagram/bus/00428185/00070170/0/'}]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_dest_dict = {}\n",
    "for lineList in lineLists:\n",
    "    soup = BeautifulSoup(str(lineList),\"html.parser\")\n",
    "    routeName = str(soup.find(\"span\",class_=\"line-name\").text)\n",
    "    dests = [{dest.text:dest[\"href\"]} for dest in soup.find_all(\"a\", class_=False)]\n",
    "    line_dest_dict[routeName] = dests\n",
    "\n",
    "line_dest_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<span><a href=\"//www.navitime.co.jp/diagram/bus/00428185/00070166/1/\">クラブメッド方面</a></span>',\n",
       " '<span><a href=\"//www.navitime.co.jp/diagram/bus/00428185/00070166/0/\">バスターミナル（石垣市）方面</a></span>']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_dest_dict['[9]川平リゾート線[東運輸]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.navitime.co.jp/bus/diagram/direction/00428185/'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平野線/平野経由伊原間線\n"
     ]
    }
   ],
   "source": [
    "text = busRoutes[0].text\n",
    "\n",
    "# \"[\"と\"]\"に囲まれた部分を消去\n",
    "cleaned_text = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ダイヤのページから,timeTableへのリンクを取得\n",
    "\"\"\"\n",
    "dir = \"https://www.navitime.co.jp\"\n",
    "diagram_link_list = []\n",
    "for l in tqdm(targetList):\n",
    "    html_text = requests.get(l).text\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    for c_dir in soup.find_all(\"ul\", class_=\"timeTable\"):\n",
    "        time.sleep(0.01)\n",
    "        diagram_link_list.append(dir+c_dir.find(\"a\").get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickleとして辞書を保存\n",
    "with open(\"C:/research/バス路線データ/scraping/時刻表_単一運用_全リンク.pickle\", \"wb\") as f:\n",
    "    pickle.dump(diagram_link_list, f)\n",
    "# read pickle pickle\n",
    "with open(\"C:/research/バス路線データ/scraping/時刻表_単一運用_全リンク.pickle\", \"rb\") as f:\n",
    "    timeTableLinkList = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ダイヤの取得\"\"\"\n",
    "for l in tqdm(timeTableLinkList):\n",
    "    html_text = requests.get(l).text\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "    route_name = soup.find(\"h2\",class_=\"head-txt\").get_text()\n",
    "    route_name = route_name.replace(\"/\",\"_\").replace(\":\",\"-\")\n",
    "    operation_name = soup.find(\"div\",class_=\"head-txt-sub\").get_text()\n",
    "    operation_name = operation_name.replace(\"/\",\"_\").replace(\":\",\"-\")\n",
    "\n",
    "    if os.path.isfile(DirTimeSchedule + \"/timeTable/\"+route_name+\"/\"+operation_name+\".csv\"): # 同データが存在する場合は以下処理はスキップ\n",
    "        continue\n",
    "    else:\n",
    "        station_name_list =[]\n",
    "        d_time_list = []\n",
    "        for element in soup.find_all(\"dl\",class_=\"stops\"):\n",
    "            station_name_list.append(element.find(\"a\",class_=\"station-name-link\").text)\n",
    "        for tElement in soup.find_all(\"dl\",class_=\"stops\"):\n",
    "            d_time_list.append(tElement.find(\"dd\",class_=\"time\").text[:5])\n",
    "        os.makedirs(\"C:/research/バス路線データ/scraping/timeTable/\"+route_name+\"/\", exist_ok=True)\n",
    "        pd.DataFrame([station_name_list,d_time_list]).T.set_axis([route_name, operation_name], axis=1).to_csv(\"C:/research/バス路線データ/scraping/timeTable/\"+route_name+\"/\"+operation_name+\".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
